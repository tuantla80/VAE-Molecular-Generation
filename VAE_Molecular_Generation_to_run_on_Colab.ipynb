{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE_Molecular_Generation_to_run_on_Colab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYcQQyBeXkRi"
      },
      "source": [
        "##**Tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpfXpYuRP0bI"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# CHARSET is receiced from preprocess.py module\n",
        "CHARSET = [' ', '#', '(', ')', '+', '-', '/', '1', '2', '3', '4', '5', '6', '7',\n",
        "           '8', '=', '@', 'B', 'C', 'F', 'H', 'I', 'N', 'O', 'P', 'S', '[', '\\\\',\n",
        "           ']', 'c', 'l', 'n', 'o', 'r', 's']\n",
        "\n",
        "\n",
        "class OneHotTokenizer():\n",
        "\n",
        "    def __init__(self, charset=CHARSET, fixed_length=120):\n",
        "        self.charset = charset\n",
        "        self.fixed_length = fixed_length\n",
        "\n",
        "    @staticmethod\n",
        "    def get_one_hot_vector(idx, N):\n",
        "        '''\n",
        "        :param idx: index in a vector\n",
        "        :param N: length of vector\n",
        "        :return: one hot vector\n",
        "            Eg. get_one_hot_vector(idx=2, N=6) -> [0, 0, 1, 0, 0, 0]\n",
        "        '''\n",
        "        return list(map(int, [idx == i for i in range(N)]))\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def get_one_hot_index(chars, char):\n",
        "        '''\n",
        "        :param chars: a list of characters or a string\n",
        "        :param char: a character\n",
        "        :return: index\n",
        "            Eg 1: get_one_hot_index(chars=CHARSET,   # CHARSET is a list above\n",
        "                                    char='#')   -> 1\n",
        "            Eg 2. get_one_hot_index(chars='Testing',  char='s') -> 2\n",
        "        '''\n",
        "        try:\n",
        "            return chars.index(char)\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "\n",
        "    def pad_smiles(self, smiles):\n",
        "        '''\n",
        "        :param smiles: Moleculer SMILES\n",
        "        :return: smiles with fixed_length\n",
        "            Eg 1. pad_smiles('banana', fixed_length=20) -> 'banana              '\n",
        "            Eg 2. pad_smiles('banana', fixed_length=2)  -> 'ba'\n",
        "            Eg 3. pad_smiles(smiles='COc(c1)cccc1C#N') ->\n",
        "                  'COc(c1)cccc1C#N                       '  # Total = 120 characters\n",
        "        '''\n",
        "        if len(smiles) <= self.fixed_length:\n",
        "            return smiles.ljust(self.fixed_length)\n",
        "        return smiles[: self.fixed_length]\n",
        "\n",
        "\n",
        "    def encode_one_hot(self, smiles):\n",
        "        '''\n",
        "        :param smiles: a molecular SMILES\n",
        "        :return:\n",
        "        Eg 1. smiles_encode = encode_one_hot(smiles='COc(c1)cccc1C#N')\n",
        "              Output: smiles_encode =\n",
        "              [  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]  # at position 1 <-> 'C'\n",
        "                 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]  # at position 1 <-> O\n",
        "                 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
        "                 [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
        "                 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
        "                 ...\n",
        "                 [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
        "                 [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
        "              ]\n",
        "              with shape = (120, 35)\n",
        "              120: length of the Smiles (Note: if shorter, make empty values at the end)\n",
        "              35: each character in smiles is coded in one-hot vector with length = length of CHARSET\n",
        "        '''\n",
        "        one_hot_indexes = [self.get_one_hot_index(chars=CHARSET, char=char) for char in self.pad_smiles(smiles)]\n",
        "        one_hot_vectors = [self.get_one_hot_vector(idx=idx, N=len(CHARSET)) for idx in one_hot_indexes]\n",
        "        return np.array(one_hot_vectors)\n",
        "\n",
        "\n",
        "    def tokenize(self, list_smiles):\n",
        "        return np.array([self.encode_one_hot(smiles) for smiles in list_smiles])\n",
        "\n",
        "\n",
        "    def decode_one_hot(self, list_encoded_smiles):\n",
        "        '''\n",
        "        :param list_encoded_smiles: list of encoded smiles getting from tokenize()\n",
        "        Eg. list_encoded_smiles =\n",
        "        [\n",
        "          # first smiles\n",
        "          [ [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]  # first character in smiles. 'C\"\n",
        "            [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]  # second character in smiles. 'O'\n",
        "            [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
        "            [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
        "            [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
        "            ...\n",
        "            [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]  # 119th character in smiles.\n",
        "            [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]  # 120th character in smiles.\n",
        "          ]\n",
        "          # second smiles\n",
        "          # third smiles\n",
        "       ]\n",
        "        z.shape = (3, 120, 35)   # the first index is number of smiles\n",
        "        :return:\n",
        "        '''\n",
        "        list_smiles_get_back = []\n",
        "        for smiles_index in range(len(list_encoded_smiles)):  # run for each smiles\n",
        "            smiles_string = ''\n",
        "            for row in range(len(list_encoded_smiles[smiles_index])):  # run for each row (each encoded character)\n",
        "                one_hot = np.argmax(list_encoded_smiles[smiles_index][row])\n",
        "                smiles_string += self.charset[one_hot]\n",
        "            # End of for row\n",
        "            list_smiles_get_back.append([smiles_string.strip()])\n",
        "        # End of for smiles_index\n",
        "        return list_smiles_get_back\n",
        "\n",
        "\n",
        "def decode_smiles_from_indexes(vec, charset=CHARSET, decode=True):\n",
        "    '''\n",
        "    :param vec:\n",
        "    :param charset: in this code the real charset is np.array\n",
        "    Eg 1. charset is CHARSET\n",
        "    charset = [' ', '#', ')', '(', '+', '-', '/', '1', '3', '2', '5',\n",
        "               '4', '7', '6', '=', '@', 'C', 'B', 'F', 'I', 'H', 'O',\n",
        "               'N', 'S', '[', ']', '\\\\', 'c', 'l', 'o', 'n', 's', 'r']\n",
        "    Eg 2. If not decode the charset may have\n",
        "    charset = [b' ' b'#' b')' b'(' b'+' b'-' b'/' b'1' b'3' b'2' b'5'\n",
        "               b'4' b'7' b'6' b'=' b'@' b'C' b'B' b'F' b'I' b'H' b'O'\n",
        "               b'N' b'S' b'[' b']' b'\\\\' b'c' b'l' b'o' b'n' b's' b'r']\n",
        "    :return:\n",
        "        Eg 1.  decode_smiles_from_indexes(vec=np.array([0, 3, 1, 2, 4, 5]),\n",
        "                                          charset='abcdef')\n",
        "           Since 'abcdef' has indexes 012345,\n",
        "           Then vec = [0, 3, 1, 2, 4, 5] will generate a string 'adbcef'\n",
        "    '''\n",
        "    if decode:\n",
        "        try:\n",
        "            charset = np.array([v.decode('utf-8') for v in charset])\n",
        "        except:\n",
        "            pass\n",
        "    # End of if\n",
        "    return ''.join(map(lambda x: charset[x], vec)).strip()\n",
        "\n",
        "\n",
        "def test():\n",
        "    one_hot_tokenizer = OneHotTokenizer(charset=CHARSET, fixed_length=120)\n",
        "    one_hot_vector = one_hot_tokenizer.get_one_hot_vector(idx=2, N=6)\n",
        "    print(f'one_hot_vector = {one_hot_vector}')\n",
        "\n",
        "    one_hot_index = one_hot_tokenizer.get_one_hot_index(chars='Testing', char='s')\n",
        "    print(f'one_hot_index = {one_hot_index}')\n",
        "\n",
        "    smiles = one_hot_tokenizer.pad_smiles(smiles='COc(c1)cccc1C#N')\n",
        "    print(f'smiles = {smiles} with length = {len(smiles)}')\n",
        "\n",
        "    smiles_encoded = one_hot_tokenizer.encode_one_hot(smiles='COc(c1)cccc1C#N')\n",
        "    np.set_printoptions(threshold=np.inf)\n",
        "    print(f'smiles_encoded = {smiles_encoded}')\n",
        "    print(f'smiles_encoded.shape = {smiles_encoded.shape}')\n",
        "\n",
        "    list_encoded_smiles = one_hot_tokenizer.tokenize(list_smiles=['COc(c1)cccc1C#N'])\n",
        "    print(f'\\ntokenizer for a list of Smiles = {list_encoded_smiles}')\n",
        "    print(f'list_encoded_smiles.shape = {list_encoded_smiles.shape}')\n",
        "\n",
        "    list_smiles_get_back = one_hot_tokenizer.decode_one_hot(list_encoded_smiles)\n",
        "    print(f'list_smiles_get_back = {list_smiles_get_back}')\n",
        "\n",
        "    print('\\ndecode smiles from indexes - testing with fake smiles')\n",
        "    fake_smiles = decode_smiles_from_indexes(vec=np.array([0, 3, 1, 2, 4, 5]),\n",
        "                                             charset='abcdef')\n",
        "    print(f'fake_smiles = {fake_smiles}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTseeUH8PrYk"
      },
      "source": [
        "##**Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFE85aNgPpRH",
        "outputId": "eb9d88c0-0275-4e99-be6b-bb842100b3a9"
      },
      "source": [
        "import torch\n",
        "import torch.utils.data\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    '''\n",
        "    Input data:\n",
        "        Shape = (batch, 120, 35)\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        self.conv_1 = nn.Conv1d(in_channels=120, out_channels=9, kernel_size=9, stride=1)\n",
        "        self.conv_2 = nn.Conv1d(in_channels=9, out_channels=9, kernel_size=9, stride=1)\n",
        "        self.conv_3 = nn.Conv1d(in_channels=9, out_channels=10, kernel_size=11, stride=1)\n",
        "\n",
        "        self.fc_0 = nn.Linear(in_features=90, out_features=435)\n",
        "        self.fc_1 = nn.Linear(in_features=435, out_features=292)\n",
        "        self.fc_2 = nn.Linear(in_features=435, out_features=292)\n",
        "        self.fc_3 = nn.Linear(in_features=292, out_features=292)\n",
        "\n",
        "        self.gru = nn.GRU(input_size=292, hidden_size=501, num_layers=3, batch_first=True)\n",
        "        self.fc_4 = nn.Linear(in_features=501, out_features=35)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax()\n",
        "\n",
        "\n",
        "    def encode(self, x):\n",
        "        '''\n",
        "        :param x:\n",
        "        :return:\n",
        "        Example\n",
        "        import numpy\n",
        "        import torch.nn as nn\n",
        "        import torch.nn.functional as F\n",
        "        import torch\n",
        "\n",
        "        batch_size = 64\n",
        "        inputs = torch.rand(batch_size, 120, 35)\n",
        "\n",
        "        # Convolutional layer\n",
        "        x = F.relu(nn.Conv1d(120, 9, kernel_size=9)(inputs)) # x.shape=torch.Size([64, 9, 27])\n",
        "        x = F.relu(nn.Conv1d(9, 9, kernel_size=9)(x))        # x.shape=torch.Size([64, 9, 19])\n",
        "        x = F.relu(nn.Conv1d(9, 10, kernel_size=11)(x))      # x.shape=torch.Size([64, 10, 9])\n",
        "\n",
        "        # fatten 2 last dimension but keep the batch_size\n",
        "        x = x.view(x.size(0), -1)                            # x.shape=torch.Size([64, 90])\n",
        "\n",
        "        # Fully connected layer\n",
        "        x = F.selu(nn.Linear(90, 435)(x))                    # x.shape=torch.Size([64, 435])\n",
        "\n",
        "        # Get z_mean and z_logvar (log-variance)\n",
        "        z_mean = nn.Linear(435, 292)(x)                      # x.shape=torch.Size([64, 292])\n",
        "        z_logvar = nn.Linear(435, 292)(x)                    # x.shape=torch.Size([64, 292])\n",
        "        '''\n",
        "        # Convolutional layer\n",
        "        x = self.relu(self.conv_1(x))\n",
        "        x = self.relu(self.conv_2(x))\n",
        "        x = self.relu(self.conv_3(x))\n",
        "\n",
        "        # Fatten 2 last dimension but keep the batch_size\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Fully connected layer\n",
        "        x = F.selu(self.fc_0(x))\n",
        "\n",
        "        # return z_mean and z_logvar\n",
        "        return self.fc_1(x), self.fc_2(x)\n",
        "\n",
        "\n",
        "    def sampling(self, z_mean, z_logvar):\n",
        "        '''\n",
        "        It is a parameterization trick to sample to get latent variable Z\n",
        "        :param z_mean: a output tensor of a standard fully connected layer from encoder (encode() function)\n",
        "        :param z_logvar: a output tensor of a standard fully connected layer from encoder (encode() function)\n",
        "        :return: z (latent variable)\n",
        "            z = z_mean + std * epsilon\n",
        "\n",
        "        Note. torch.randn_like(input): Returns a tensor with the same size as input that\n",
        "              is filled with random numbers from a normal distribution with mean 0 and\n",
        "              variance 1. Therefore, input here is just to get shape.\n",
        "\n",
        "        Example: continue with example in encode() method\n",
        "        std = torch.exp(0.5 * z_logvar)                 # std.shape=torch.Size([64, 292])\n",
        "        epsilon = 1e-2 * torch.randn_like(input=std)  # epsilon.shape=torch.Size([64, 292])\n",
        "        z = z_mean + std * epsilon                        # z.shape=torch.Size([64, 292])\n",
        "        '''\n",
        "        std = torch.exp(0.5 * z_logvar)\n",
        "        epsilon = 1e-2 * torch.randn_like(input=std)  # multiply 1e-2 to make epsilon smaller\n",
        "        return  z_mean + std * epsilon\n",
        "\n",
        "\n",
        "    def decode(self, z):\n",
        "        '''\n",
        "        :param z:\n",
        "        :return:\n",
        "\n",
        "        Example: continue with example in sampling() method\n",
        "        z = F.selu(nn.Linear(292, 292)(z))                      # z.shape=torch.Size([64, 292])\n",
        "        z = z.view(z.size(0), 1, z.size(-1)).repeat(1, 120, 1)  # z.shape=torch.Size([64, 120, 292])\n",
        "        output, h_n = nn.GRU(292, 501,\n",
        "                             num_layers=3,\n",
        "                             batch_first=True)(z)               # output.shape=torch.Size([64, 120, 501])\n",
        "                                                                # h_n.shape=torch.Size([3, 64, 501])\n",
        "        out_reshape = output.contiguous()\n",
        "                            .view(-1, output.size(-1))          # out_reshape=torch.Size([7680, 501]) # 7680=64*120\n",
        "\n",
        "        y_out = nn.Linear(501, 35)(out_reshape)                 # y_out.shape=torch.Size([7680, 35])\n",
        "        y_out = F.softmax(y_out, dim=1)                         # y_out.shape=torch.Size([7680, 35])\n",
        "                                                                # dim=1 -> sum to 1 to every row\n",
        "        y = y_out.contiguous()\n",
        "                 .view(output.size(0), -1, y_out.size(-1))      # y.shape=torch.Size([64, 120, 35])\n",
        "        '''\n",
        "        z = F.selu(self.fc_3(z))\n",
        "        z = z.view(z.size(0), 1, z.size(-1)).repeat(1, 120, 1)\n",
        "        output, h_n = self.gru(z)\n",
        "        output_reshape = output.contiguous().view(-1, output.size(-1))\n",
        "        y_out = F.softmax(self.fc_4(output_reshape), dim=1)\n",
        "        y = y_out.contiguous().view(output.size(0), -1, y_out.size(-1))\n",
        "        return y\n",
        "    \n",
        "    \n",
        "    def forward(self, x):\n",
        "        z_mean, z_logvar = self.encode(x)\n",
        "        z = self.sampling(z_mean, z_logvar)\n",
        "        y = self.decode(z)\n",
        "        return y, z_mean, z_logvar\n",
        "\n",
        "\n",
        "def test_class_VAE():\n",
        "    batch = 64\n",
        "    inputs = torch.rand(batch, 120, 35)\n",
        "    y, z_mean, z_logvar = VAE().forward(x=inputs)\n",
        "    print(f'output: y.shape = {y.shape}')\n",
        "    print(f'latent space: z_mean.shape = {z_mean.shape}')\n",
        "    print(f'latent space: z_logvar.shape = {z_logvar.shape}')\n",
        "        \n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print('Run a test for forward VAE')\n",
        "    test_class_VAE()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Run a test for forward VAE\n",
            "output: y.shape = torch.Size([64, 120, 35])\n",
            "latent space: z_mean.shape = torch.Size([64, 292])\n",
            "latent space: z_logvar.shape = torch.Size([64, 292])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WblTy8W9QQ7s"
      },
      "source": [
        "## **Google drive mount**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-I-y3aB505g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e49c8e4f-b57f-4bef-acf0-098ade577a4a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# path: \"metadata.csv\" and \"embeddings.csv\" files are stored here\n",
        "path = '/content/drive/My Drive/Colab Notebooks'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.activity.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fexperimentsandconfigs%20https%3a%2f%2fwww.googleapis.com%2fauth%2fphotos.native&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "4/1AY0e-g4bvnEx6NuEpNPB3qjfAXSQnRxEeNNRX5-weW7a51z96f-MIFE925E\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcp-WsqU6dzn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4c5f4cb8-7124-47cf-eb10-959bedb6b92b"
      },
      "source": [
        "path"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/Colab Notebooks'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yh2fbeSsRi9D"
      },
      "source": [
        "##**Train model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJxOMqLu5cHL"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# from models import VAE\n",
        "# from tokenizer import OneHotTokenizer\n",
        "\n",
        "\n",
        "def vae_loss(x_reconstructed, x, z_mean, z_logvar):\n",
        "    bce_loss = F.binary_cross_entropy(input=x_reconstructed, target=x, reduction='sum')\n",
        "    kl_loss = -0.5 * torch.sum(1 + z_logvar - z_mean.pow(2) - z_logvar.exp())\n",
        "    return bce_loss + kl_loss\n",
        "\n",
        "\n",
        "def train(file_path_train_data=r'data\\smiles_tokenized.npz',\n",
        "          path_checkpoint=None,\n",
        "          checkpoint_save='every',\n",
        "          file_path_checkpoint_for_continue_learning=None,\n",
        "          batch_size=1000,\n",
        "          epochs=100):\n",
        "    '''\n",
        "    :param file_path_train_data: file path of data after preprocessing\n",
        "        Eg. r'data\\smiles_tokenized_10000.npz' or r'data\\smiles_tokenized.npz'\n",
        "    :param path_checkpoint:\n",
        "        if existed, save model check point.\n",
        "        Eg. checkpoint = {\n",
        "                'epoch': epoch,\n",
        "                'model': model.state_dict(),\n",
        "                'optimizer': optimizer.state_dict()\n",
        "            }\n",
        "    :param checkpoint_save: only activate when path_checkpoint is existed\n",
        "        checkpoint_save = 'every': save every epoch\n",
        "                        = 'last': save only final epoch\n",
        "                        = a number: every 'a number' epoch. eg. every 5 epoch\n",
        "\n",
        "    :param file_path_checkpoint_for_continue_learning:\n",
        "        if existed, read the model and optimizer parameters as starting point for training\n",
        "                    (instead of training from the initial state)\n",
        "    :param batch_size:\n",
        "    :param epochs:\n",
        "    :return:\n",
        "    '''\n",
        "    assert checkpoint_save == 'every' or checkpoint_save == 'last' or isinstance(checkpoint_save, int)\n",
        "    # Get data\n",
        "    train_data = np.load(file_path_train_data)['arr'].astype(np.float32) # Eg. X.shape = (249456, 120, 35)\n",
        "    train_data = torch.utils.data.TensorDataset(torch.from_numpy(train_data))\n",
        "    train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Model\n",
        "    torch.manual_seed(42)\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(f'device = {device}')\n",
        "    model = VAE()\n",
        "    model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    start_epoch, stop_epoch = 0, epochs\n",
        "    if file_path_checkpoint_for_continue_learning:\n",
        "        checkpoint = torch.load(file_path_checkpoint_for_continue_learning, map_location=device)\n",
        "        model.load_state_dict(checkpoint['model'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        start_epoch, stop_epoch = checkpoint['epoch'] + 1, checkpoint['epoch'] + epochs + 1\n",
        "    # End of if\n",
        "\n",
        "    # model.to(device)\n",
        "\n",
        "    for epoch in range(start_epoch, stop_epoch):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for batch_idx, data in enumerate(train_data_loader):\n",
        "            data = data[0].to(device)  # Note: data is a list of one 'element'\n",
        "            optimizer.zero_grad()  # reset - zero out gradient\n",
        "            \n",
        "            # Forward process: compute output (prediction) and loss\n",
        "            output, z_mean, z_logvar = model(data)\n",
        "            loss = vae_loss(output, data, z_mean, z_logvar)\n",
        "            \n",
        "            # Backward process: compute gradients\n",
        "            loss.backward()\n",
        "            \n",
        "            # Update parameters\n",
        "            optimizer.step()\n",
        "\n",
        "            # Display some info\n",
        "            train_loss += loss\n",
        "            if batch_idx % 100 == 0:\n",
        "                print(f'\\nepoch/batch_idx: {epoch}/{batch_idx}\\t loss = {loss: .4f}')\n",
        "                # Input data\n",
        "                input_data = data.cpu().numpy()\n",
        "                print(f'\\tFor testing: The first input smiles of batch={batch_size} Smiles')\n",
        "                print('\\t', OneHotTokenizer().decode_one_hot(list_encoded_smiles=[input_data[0]]))\n",
        "                # Output data\n",
        "                output_data = output.cpu().detach().numpy()\n",
        "                print(f'\\tFor testing: The first output smiles of {len(output_data)} generated Smiles')\n",
        "                print('\\t', OneHotTokenizer().decode_one_hot(list_encoded_smiles=[output_data[0]]))\n",
        "        # End of for batch_idx,...\n",
        "        train_loss /= len(train_data_loader.dataset)\n",
        "        print(f'Average train loss of this epoch = {train_loss}')\n",
        "\n",
        "        if path_checkpoint:\n",
        "            checkpoint = {\n",
        "                'epoch': epoch,\n",
        "                'model': model.state_dict(),\n",
        "                'optimizer': optimizer.state_dict()\n",
        "            }\n",
        "            if checkpoint_save == 'every':\n",
        "                torch.save(obj=checkpoint, f=os.path.join(path_checkpoint, fr'checkpoint_{epoch}.pt'))\n",
        "            elif checkpoint_save == 'last':\n",
        "                if epoch == stop_epoch - 1:\n",
        "                    torch.save(obj=checkpoint, f=os.path.join(path_checkpoint, fr'checkpoint_{epoch}.pt'))\n",
        "            else:\n",
        "                if epoch % checkpoint_save == 0:\n",
        "                    torch.save(obj=checkpoint, f=os.path.join(path_checkpoint, fr'checkpoint_{epoch}.pt'))\n",
        "        # End of if path_checkpoint:\n",
        "    # End of for epoch\n",
        "    return train_loss"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wdMZNG05Mrz"
      },
      "source": [
        "##**Traing for the first 20 epochs**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBUrx72Dn92D"
      },
      "source": [
        "# if __name__ == '__main__':\n",
        "#     train_loss = train(file_path_train_data=os.path.join(path, 'smiles_tokenized.npz'),\n",
        "#                        path_checkpoint=path,\n",
        "#                        checkpoint_save='last',\n",
        "#                        file_path_checkpoint_for_continue_learning=None,  # r'checkpoint_3.pt',\n",
        "#                        batch_size=1000,\n",
        "#                        epochs=20)\n",
        "\n",
        "#     print(f'train_loss = ', train_loss)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZKsZBPU33rs"
      },
      "source": [
        "##**Continue learning: Training for the next 20 epochs**\n",
        "From epochs=20 to 39 (Traing of 20 epochs (0 to 19) has been done above)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jeSCYlCnQWs"
      },
      "source": [
        "# if __name__ == '__main__':\n",
        "#     time_start = datetime.now()\n",
        "#     print(f'time_start = {time_start}')\n",
        "#     train_loss = train(file_path_train_data=os.path.join(path, 'smiles_tokenized.npz'),\n",
        "#                        path_checkpoint=path,\n",
        "#                        checkpoint_save='last',\n",
        "#                        file_path_checkpoint_for_continue_learning=os.path.join(path, 'checkpoint_19.pt'),\n",
        "#                        batch_size=1000,\n",
        "#                        epochs=20)\n",
        "#     print(f'train_loss = ', train_loss)\n",
        "#     time_end = datetime.now()\n",
        "#     print(f'time_end = {time_end}')\n",
        "#     print(f'Total running time = {time_end - time_start}')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avHIyv7MoFVx"
      },
      "source": [
        "## **Continue learning: Training for the next 60 epochs**\n",
        "Just change params of the above code to run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3yXwsIjptbK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}